{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil, time, sys, pdb\n",
    "import numpy as np, pandas as pd, time, os, shutil, sys\n",
    "from os.path import join\n",
    "\n",
    "from torchtext.data import Field, TabularDataset, BucketIterator\n",
    "import torch, torchtext\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torchtext.vocab import Vectors, GloVe\n",
    "from torchtext.datasets import IMDB\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_trainer.engine import Model\n",
    "from torch_trainer.metrics import accuracy\n",
    "from torch_trainer.callbacks import CyclicalLearningRate, ModelCheckpoint\n",
    "from torch_trainer.callbacks.logger import NeptuneLogger\n",
    "from torch_trainer.utils.lr_finder import LRFinder\n",
    "from torch_trainer.optimizers import RAdam, Lookahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = Field(lower=True, include_lengths=True, batch_first=True, fix_length=90)\n",
    "LABEL = Field(sequential=False)\n",
    "\n",
    "# make splits for data\n",
    "train, test = IMDB.splits(TEXT, LABEL)\n",
    "\n",
    "# build the vocabulary\n",
    "MAX_VOCAB_SIZE = 25000\n",
    "TEXT.build_vocab(train, vectors=GloVe(name='6B', dim=100), max_size = MAX_VOCAB_SIZE, unk_init = torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train)\n",
    "vocab_size = len(TEXT.vocab)\n",
    "\n",
    "# make iterator for splits\n",
    "train_iter, test_iter = BucketIterator.splits((train, test), batch_size=256, device=torch.device('cpu'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_size = 100\n",
    "hidden_size = 256\n",
    "output_size = 2\n",
    "\n",
    "class GRUClassifier(Model):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors, requires_grad=True)\n",
    "        \n",
    "        self.gru = nn.GRU(embedding_size, hidden_size, batch_first=False, bidirectional=True, dropout=0.4, num_layers=2)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.label = nn.Linear(hidden_size*4, output_size)    \n",
    "        \n",
    "    def forward(self, z):\n",
    "        z = self.dropout(self.embedding(z).transpose(0,1))\n",
    "        out, h_state = self.gru(z)\n",
    "        concat_hidden = torch.flatten(h_state.transpose(0,1), start_dim=1, end_dim=2)\n",
    "#         logits = self.label(concat_hidden)\n",
    "        logits = self.label(self.dropout(concat_hidden))\n",
    "        return logits\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_input(batch):\n",
    "        return batch.text[0], batch.label-1\n",
    "\n",
    "class RNN(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_size)\n",
    "        self.embedding.weight = nn.Parameter(TEXT.vocab.vectors, requires_grad=True)\n",
    "        self.rnn = nn.LSTM(embedding_size, \n",
    "                           hidden_size, \n",
    "                           num_layers=2, \n",
    "                           bidirectional=True, \n",
    "                           dropout=0.5)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        \n",
    "    def forward(self, text):\n",
    "        embedded = self.dropout(self.embedding(text).transpose(0,1))\n",
    "        packed_output, (hidden, cell) = self.rnn(embedded)\n",
    "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1))\n",
    "        return self.fc(hidden)\n",
    "    \n",
    "    @staticmethod\n",
    "    def parse_input(batch):\n",
    "        return batch.text[0], batch.label-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = GRUClassifier()\n",
    "model = RNN()\n",
    "opt = optim.Adam(model.parameters())\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "model.compile(optimizer=opt, metrics=[accuracy], loss=criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpkt = ModelCheckpoint('models/test.pth', monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True)\n",
    "cyclic_lr = CyclicalLearningRate(base_lr=2e-3, max_lr=5e-2, epochs_per_cycle=3, auto_find_lr=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/5\n",
      "96/98 [============================>.] - ETA: 0s - loss: 0.6558 - accuracy: 67.5781"
     ]
    }
   ],
   "source": [
    "hist = model.fit(train_iter, epochs=5, val_dataloader=test_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
